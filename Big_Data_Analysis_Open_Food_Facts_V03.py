# -*- coding: utf-8 -*-
"""The Glam Guys.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KNag-uIyJwU9gNpITqiCP2hBsp4f-I5n

# Introduction / Background

**Introduction:**

Open Food Facts is a non-profit collaborative project that aims to create a comprehensive, open-source database of food products from around the world. The project is managed by a non-profit organization with over a hundred thousand active contributors. The fact that the database is open source means that anyone can volunteer and contribute. Currently, the database contains entries of over 3 million food products from 150 countries with information pertaining to the ingredients, allergens, and nutrition facts found on product labels. The data is collected from a variety of sources, including barcode scans, product labels, and volunteer contributions.


**Background on the dataset:**

The Open Food Facts database was created out of frustration with the lack of transparency in the food industry. Because food manufacturers often use proprietary ingredients and complex labeling, many consumers are ill-informed when it comes to knowing what is in their food. Thus, the Open Food Facts project aims to provide transparency to consumers with easy to access product information that is accurate and up-to-date. ðŸ™‚


**Project Objective:**

Using the Open Food Facts database, we want to achieve the following objectives in this project:

1.  Predict the UK Nutrition score categories of food products by using Linear regression and Logistic Regression models
2.   Group different food products based on their ingredients by using Decision Tree

Before starting the analysis, we import the necessary packages as follows:

Loading Dependencies
"""

!pip install torchinfo
!pip install dtreeviz

# Commented out IPython magic to ensure Python compatibility.
# import packages

# to mount google drive
from google.colab import drive

# For data analysis
import pandas as pd
import numpy as np

# For data visualization
from matplotlib import pyplot as plt
# %matplotlib inline
import seaborn as sns

# sklearn libraries
import sklearn
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
import sklearn.metrics
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import confusion_matrix
from sklearn import tree
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree

import matplotlib.pyplot as plt

# resampling package
from imblearn.over_sampling import RandomOverSampler

import torch
import torchvision
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchinfo import summary
from torch.utils.data import random_split

# Setting the device to do computations on - GPU's are generally faster!
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(torch.__version__)
print(device)

"""Mount Google Drive, import data and read data into a Panda DataFrame"""

# drive.flush_and_unmount()
drive.mount('/content/drive')
#df = pd.read_table('/content/drive/MyDrive/en.openfoodfacts.org.products.tsv')
df = pd.read_table('/content/drive/MyDrive/5450/en.openfoodfacts.org.products.tsv')
# df = pd.read_table('/content/drive/MyDrive/kaggle/en.openfoodfacts.org.products.tsv')

"""# Exploratory Data Analysis

The Open Food Facts dataset is quite large with a size of about 1 GB (~360000 entries!). So, it would make sense to first conduct EDA to understand what we are working with. To have an idea, let us take a look at the first 10 rows of our dataframe df.

"""

# Peek first 10 rows of the data set
df.head(10)

"""## What are the data types?
From our initial observations, we can see that the dataframe contains information on a variety of food products, including soft drinks, teas, and other beverages. The products are from a variety of countries, including the United States, Germany, Spain, the United Kingdom, and France. The dataframe also includes information on the nutrients in each product, such as energy, sugar, and fat per 100 grams. Likewise, there are columns that list the ingredients and allergens (if applicable) for each product. On the other hand, there are plenty of columns that the Open Food Facts website uses as part of their backend code such as when a product entry was created ("created_datetime") or when it was last modified ("last_modified_datetime"). These columns are not particularly useful to us. However, columns like ("product_name") and ("ingredients_text") will definitely be of interest.

Next, we shall take a look at what sort of data types we are dealing with in the columns of this dataframe.
"""

# Check data types of columns and the shape of dataframe
df.dtypes

"""## What is the shape?
We can observe that the data types are quite diverse, consisting of objects and float64. Since a majority of the columns contain object values, this indicates that the dataset incorporates a variety of data formats, including strings (product name, categorie, country), numerical data (nutrients per 100 grams), and even comma-separated values (ingredients).

The presence of comma-separated values within the "ingredients_text" column suggests that the nutritional information for each product is some what structured and organized, allowing us the potential to analyze and explore nutritional patterns and trends. As a result, this means we will need to cast the data types appropriately so that it can be parsed.

Now, we will take a look at the shape of this dataframe and the descriptive summary.
"""

# Get the shape of the dataframe
df.shape

"""## A descriptive summary of our dataframe
As alluded to earlier in our introduction, the Open Food Facts dataset is quite large and comprehensive with a shape that is an astonishing 356027 rows and 163 columns! This tells us that the dataset has a significant amount of information on a wide range of food products. The large number of rows implies that the dataset can potentially provide detailed analysis and insights into the various aspects of food products, such as their origins, pricing, and nutritional composition. Likewise, the large number of columns suggests that the dataset covers a broad range of attributes related to each food product.

Then, we shall take a look at the descriptive summary of this dataframe.
"""

# Get a descriptive summary of the dataframe
df.describe()

"""## What columns do we have?
Since there are 163 columns in the dataframe, there is no way we can display them all at once when viewing the dataframe. So, one way is to iterate all the columns in the dataframe and print them out.
"""

# Look through all the available columns in the dataset
for col in df.columns:
  print(col)

"""## Selecting relevant columns

Selecting relevant columns is a crucial step to ensure that the analysis focuses on the most informative and pertinent data. Given our logistic regression and clustering tasks, we are primarily interested in the ingredients of the food products, as these provide insights into the nutritional properties and potential health implications of the products. Therefore, we are dropping columns that are not useful for the tasks such as data entry and packaging information; only keeping ingredients-related columns.

By focusing on ingredients-related columns, we can effectively streamline the analysis process, reducing computational complexity and improving the efficiency of our models. Moreover, excluding irrelevant columns helps to minimize noise and potential biases that may arise from unrelated data, enhancing the accuracy and reliability of our results.

We will create a new dataframe called new_df by selecting the columns we want. Then, we will cast the columns containing objects to string data type.
"""

# Select relevant columns
new_df = df[['product_name', 'brands','brands_tags', 'countries_en', 'ingredients_text', 'allergens', 'additives_n', 'additives_en', 'energy_100g', 'fat_100g', 'saturated-fat_100g', 'monounsaturated-fat_100g', 'polyunsaturated-fat_100g', 'omega-3-fat_100g', 'omega-6-fat_100g', 'omega-9-fat_100g', 'trans-fat_100g', 'cholesterol_100g', 'carbohydrates_100g', 'sugars_100g', 'starch_100g', 'polyols_100g', 'fiber_100g', 'proteins_100g', 'salt_100g', 'fruits-vegetables-nuts_100g', 'nutrition-score-uk_100g']]

# Cast types of columns and confirm change
new_df['product_name'] = new_df['product_name'].astype('string')
new_df['brands'] = new_df['brands'].astype('string')
new_df['brands_tags'] = new_df['brands_tags'].astype('string')
new_df['countries_en'] = new_df['countries_en'].astype('string')
new_df['ingredients_text'] = new_df['ingredients_text'].astype('string')
new_df['allergens'] = new_df['allergens'].astype('string')
new_df['additives_en'] = new_df['additives_en'].astype('string')
new_df = new_df.sort_values(by='additives_n', ascending=False)

"""We can now take a quick look at the result of our new dataframe."""

# Check the data types for the columns
new_df.dtypes

# Sense check cleaned df
new_df.info()

"""At this point, it makes sense to take a look at the first 10 rows of our dataframe new_df to get a sense of how things look. We have not dropped any null values yet, so it is expected that there will be some rows with this problem."""

# Print the first 10 rows of new_df
new_df.head(10)

"""To limit ourselves on English data, we narrow down the dataset to only entries from the United States and where the ingredient list is not empty."""

# Filter the rows for products that are in the United States
new_1_df = new_df[new_df['countries_en'] == 'United States']

# Drop any rows that contain null as the ingredients
new_1_df.dropna(subset=['ingredients_text'], inplace=True)

# Display the data frame new_1_df
new_1_df

"""With our dataframe new_1_df containing the information needed, we are ready to move onto cleaning and wrangling the data!

# Data Cleaning and Wrangling
We took a comprehensive approach to data cleaning and wrangling, addressing common data quality issues and preparing the data for subsequent analyses. We achieve this in the following steps:

**Cleaned Dataframe: df_cleaned_V1** size:(170551, 29)

1.   Checking NaN distribution among all columns;\
The first step is to identify and handle any missing values (NaNs) across all columns.
2.   Padding numerical columns with zeros for all NaN entries;\
We check the NaN distribution across all columns and pad any numerical columns with zeros for NaN entries. This ensures that numerical columns have consistent representations, preventing errors in computations and ensuring effective ML training.

3.   Remove rows with NaNs in alphabatical entries;\
For string-based columns, we remove rows with NaNs in alphabetical entries. This helps maintain data integrity and prevent any inconsistencies that could arise from incomplete or inaccurate information.
4.   Convert Non-English letters to English letters in alphabatical columns;\
Typically, non-English characters are common in string-based columns. For this reason, we convert any non-English letters to English letters in alphabetical columns. This ensures consistent data representation and facilitates accurate text analysis.

5.   Expand ingredients column into several one-hot coding columns padding with 1s and 0s;\
We will transform the "ingredients_text" column into one-hot coding columns, effectively representing the data in a numerical format suitable for ML training. This step allows the ingredients information to be incorporated into the analysis in a structured and interpretable manner.

6.   Rename some column names and ready data for following ML training and testing.
We will rename the column names to be more descriptive and meaningful, which will enhance the readability and interpretability of the data. This improves the clarity of the data and facilitates easier understanding for subsequent analyses.

We begin with step 1 by checking the distribution of NaN values across all of the columns.
"""

# Check NaNs value's means
new_1_df.isnull().mean(axis=0)

"""## Step 1: Proportion of NaN values
The proportion of NaN values can be calculated by getting the mean for NaN values in the dataset. The resulting percentage represents the proportion of data points that are missing for that particular column.

With respect to the Open Food Facts dataset, examining the proportion of NaN values can help us understand the completeness of the information provided for various product attributes. For example, if a significant proportion of values are missing for a column like "ingredients_text", it may indicate that the data collection process needs improvement or that there are limitations in the availability of ingredient information for some products.

Thus, to help us quickly identify columns with high proportions of missing data, we use visualization techniques such bar charts to represent these proportions.
"""

# check the proportion of NaNs in each column
plt.figure(figsize=(5, 10))
new_1_df.isnull().mean(axis=0).plot.barh()
plt.title("Proportion of NaNs in each column")

"""### Why is there missing data?
Upon examining the bar chart above, we can see that several columns exhibit a high proportion of NaN values. These columns include "allergens", "omega-3-fat_100g", "omega-6-fat_100g", "omega-9-fat_100g", "starch_100g", "polyols_100g", and "fruits-vegetables-nuts_100g".


The high proportion of NaN values in the "allergens" column suggests that allergen information is not consistently provided for all products. We believe this is due to variations in product labeling practices or a lack of standardized allergen reporting guidelines. Likewise, the missing values in the omega fatty acid columns "omega-3-fat_100g", "omega-6-fat_100g", and "omega-9-fat_100g" suggests that information about omega fatty acid content is not always available. This could be due to the complexity of measuring omega fatty acids or a lack of emphasis on this nutritional aspect by some manufacturers. Similarly, the absence of starch content data in the "starch_100g" column suggests that this information may not be routinely collected or reported for all food products. Perhaps, there are variations in starch content across different product types or a lack of standardized starch measurement protocols. Overall, we believe there are no standard guidelines when it comes to product labelling practices, which may contribute to the inconsistencies in the dataset.


Thus, the presence of missing data in these columns have many implications for the analysis and interpretation of the Open Food Facts dataset. If we aim to analyze the prevalence of allergens or the nutritional composition of products, the missing values could introduce biases and limit the accuracy of our findings. For this reason, we will drop columns with 100% NaN values.
"""

new_1_df.columns

# drop columns with 100% NaNs
useless_features = ["fruits-vegetables-nuts_100g", "polyols_100g", "starch_100g", "omega-3-fat_100g", "omega-6-fat_100g", "omega-9-fat_100g", "allergens"]
df_cleaned = new_1_df.drop(useless_features, axis=1, inplace=False)

# Sense check new dataframe df_cleaned
print(df_cleaned.shape)
print(df_cleaned.columns)

"""## Step 2: Replacing NaN values with zeroes appropriately
We assume that NaN values in some of the columns represent the absence of the corresponding ingredients. We feel this is a reasonable assumption given the context of the dataset. Since the Open Food Facts dataset aims to provide comprehensive information about food products, it is likely the NaN values indicate that the ingredients are not present rather than missing due to data collection errors or inconsistencies.

Replacing NaN values with zeroes effectively imputes these missing values by assuming that the ingredient is absent and therefore quantifying its presence as zero. This approach is particularly useful for numerical columns representing quantities of ingredients, such as "fat_100g" or "sugars_100g".

By replacing NaN values with zeroes, we are able to incorporate these columns into our analysis. This allows us to make more informed comparisons between products and identify patterns in ingredient usage. However, we acknowledge that this strategy assumes that the absence of an ingredient is the only possible explanation for a NaN value.

As for the target column "nutrition-score-uk_100g", we will drop the rows with mising value as we cannot ascertain what the actual values are we cannot assume that the value is zero.
"""

# replace NaNs with zeros in numerical columns
df_cleaned["additives_n"] = df_cleaned["additives_n"].fillna(0)
df_cleaned["energy_100g"] = df_cleaned["energy_100g"].fillna(0)
df_cleaned["fat_100g"] = df_cleaned["fat_100g"].fillna(0)
df_cleaned["saturated-fat_100g"] = df_cleaned["saturated-fat_100g"].fillna(0)
df_cleaned["monounsaturated-fat_100g"] = df_cleaned["monounsaturated-fat_100g"].fillna(0)
df_cleaned["polyunsaturated-fat_100g"] = df_cleaned["polyunsaturated-fat_100g"].fillna(0)
df_cleaned["trans-fat_100g"] = df_cleaned["trans-fat_100g"].fillna(0)
df_cleaned["cholesterol_100g"] = df_cleaned["cholesterol_100g"].fillna(0)
df_cleaned["carbohydrates_100g"] = df_cleaned["carbohydrates_100g"].fillna(0)
df_cleaned["sugars_100g"] = df_cleaned["sugars_100g"].fillna(0)
df_cleaned["fiber_100g"] = df_cleaned["fiber_100g"].fillna(0)
df_cleaned["proteins_100g"] = df_cleaned["proteins_100g"].fillna(0)
df_cleaned["salt_100g"] = df_cleaned["salt_100g"].fillna(0)
df_cleaned = df_cleaned[df_cleaned['nutrition-score-uk_100g'].notna()]
df_cleaned.head()

"""### A second look at the columns
After zeroing out the NaN values in the respective columns, we shall take a second look at the bar chart to confirm the change.
"""

# check the proportion of NaNs in each column
plt.figure(figsize=(5, 10))
df_cleaned.isnull().mean(axis=0).plot.barh()
plt.title("Proportion of NaNs in each column")

"""## Step 3: Addressing remaning NaN values
After zeroing out the NaN values in the respective columns, the bar chart above shows a significant reduction in missing values with the exception of a few columns, namely the "additives_en", "brand_tags", and "brands" columns.

The "additives_en" column represents the what additives are in a product. Since this column contains non-numerical data, imputing NaN values with zeroes is not appropriate. Moreover, the additives are not directly related to the ingredients themselves, making it less relevant for our analysis of product composition. Therefore, we decide to drop the "additives_en" column to avoid potential biases or misinterpretations. This ensures that our findings are primarily driven by the ingredients themselves, minimizing the influence of external factors such as additives.
"""

# drop additives_en column, over 35% data is NaNs
useless_features = ["additives_en"]
df_cleaned = df_cleaned.drop(useless_features, axis=1, inplace=False)

"""Now, we will eradicate any remaining NaN values in all columns once and for all."""

# drop rows containing NaNs
df_cleaned = df_cleaned.dropna()

# check for NaNs, complete.
plt.figure(figsize=(5, 10))
df_cleaned.isnull().mean(axis=0).plot.barh()
plt.title("Proportion of NaNs in each column")

"""As we can see in the bar chart above, there are no more NaN values. Things are looking good, so we will take a further look at the end result."""

# check for cleaned dataset dimensions
print(df_cleaned.shape)
df_cleaned.head()

"""By removing all of the remaining NaN values, we have enhanced the quality and integrity of our dataset. Likewise, the shape of our dataframe has changed from (356027, 163) to (138025, 19). This reduction in both the number of rows and columns highlights the effectiveness of our data cleaning process. We removed irrelevant columns and addressed missing values, resulting in a streamlined and focused dataset that is more suited for our analysis."""

!pip install easynmt

"""## Step 4: Converting non-english letters to english letters

The presence of non-English letters in string columns can be problematic and hinder the training of our machine learning models, which rely on character-level processing. By converting non-English letters to English letters, we can standardize the textual data to ensure that all characters are understood and processed consistently. Likewise, product tags play a crucial role in product categorization and information retrieval in real life applications. By converting all tags to lowercase, we eliminate the potential for inconsistencies with tags that have capitalized characters in them.

"""

# convert all non-english letters into english letters in alphabatical columns
# convert all tags into lower-case, in order for following ML training use
df_cleaned_V1 = df_cleaned.copy()
df_cleaned_V1["product_name"] = df_cleaned_V1["product_name"].str.normalize('NFKD').str.encode('ascii',errors='ignore').str.decode('utf-8').str.lower()
df_cleaned_V1["brands"] = df_cleaned_V1["brands"].str.normalize('NFKD').str.encode('ascii',errors='ignore').str.decode('utf-8').str.lower()
df_cleaned_V1["brands_tags"] = df_cleaned_V1["brands_tags"].str.normalize('NFKD').str.encode('ascii',errors='ignore').str.decode('utf-8').str.lower()
df_cleaned_V1["countries_en"] = df_cleaned_V1["countries_en"].str.normalize('NFKD').str.encode('ascii',errors='ignore').str.decode('utf-8').str.lower()
df_cleaned_V1["ingredients_text"] = df_cleaned_V1["ingredients_text"].str.normalize('NFKD').str.encode('ascii',errors='ignore').str.decode('utf-8').str.lower()

df_cleaned_V1.head()

"""After carrying out the mentioned tasks, we take a look at the first 10 rows of our data to see how things are turning out. It appears that everything is fine."""

# examine the difference between the 'brands' and 'brands_tags' column
np.sum(df_cleaned_V1["brands"]!=df_cleaned_V1["brands_tags"])

"""Since both the "brands" and "brands_tags" are duplicates of each other, there is no need to have both. For this reason, we will drop the "brands_tags" column.

"""

# drop brands_tags column, it's same with brands column
useless_features = ["brands_tags"]
df_cleaned_V1 = df_cleaned_V1.drop(useless_features, axis=1, inplace=False)
df_cleaned_V1.head()

"""##Step 6: Simplifying column names

To enhance the clarity and readability of our dataset, we have renamed the columns: "countries_en" to "countries", "ingredients_text" to "ingredients", and "additives_n" to "additives".

The reason we chose to rename "countries_en" is because "en" indicates that the column contains country names in English. By shortening the name to simply "countries," we provide a more concise and straightforward label. Likewise, we rename "ingredients_text" because "text" implies that the column contains textual information about the ingredients. Renaming it to "ingredients" directly conveys the column's content without being redundant. And last, "additives_n" suggests that the column contains information about the additives in a product. We felt renaming it to "additives" is more straightforward. Ultimately, we want to make the dataset more user-friendly and accessible by removing any extraneous semantics.


"""

# rename some columns
df_cleaned_V1 = df_cleaned_V1.rename(columns={"countries_en": "countries", "ingredients_text": "ingredients", "additives_n": "additives"})

# explode ingredients column
# add columns indicating whether some ingredients are in this product
# for future ML use, feel free to join more columns here
# list(df_cleaned_V1['ingredients'].str.split(' ', expand=True).stack().unique())
df_cleaned_V1["has_flour"] = df_cleaned_V1["ingredients"].str.contains("flour").apply(lambda x: 1 if x==True else 0)
df_cleaned_V1["has_sugar"] = df_cleaned_V1["ingredients"].str.contains("sugar").apply(lambda x: 1 if x==True else 0)
df_cleaned_V1["has_water"] = df_cleaned_V1["ingredients"].str.contains("water").apply(lambda x: 1 if x==True else 0)
df_cleaned_V1["has_salt"] = df_cleaned_V1["ingredients"].str.contains("salt").apply(lambda x: 1 if x==True else 0)
df_cleaned_V1["has_potassium"] = df_cleaned_V1["ingredients"].str.contains("potassium").apply(lambda x: 1 if x==True else 0)
df_cleaned_V1["has_calcium"] = df_cleaned_V1["ingredients"].str.contains("calcium").apply(lambda x: 1 if x==True else 0)
df_cleaned_V1["has_acid"] = df_cleaned_V1["ingredients"].str.contains("acid").apply(lambda x: 1 if x==True else 0)
df_cleaned_V1["has_alkali"] = df_cleaned_V1["ingredients"].str.contains("alkali").apply(lambda x: 1 if x==True else 0)
df_cleaned_V1["has_iodine"] = df_cleaned_V1["ingredients"].str.contains("iodine").apply(lambda x: 1 if x==True else 0)
df_cleaned_V1["has_milk"] = df_cleaned_V1["ingredients"].str.contains("milk").apply(lambda x: 1 if x==True else 0)
df_cleaned_V1["has_oil"] = df_cleaned_V1["ingredients"].str.contains("oil").apply(lambda x: 1 if x==True else 0)
df_cleaned_V1.head()

"""Examining the printed result above, we see that things are looking fantastic."""

# print the shape of the dataframe
print(df_cleaned_V1.shape)

# data ready for modeling
print(df_cleaned_V1.dtypes)

"""After a thorough data cleaning and wrangling process, our dataframe now presents a clean, consistent, and ready-to-use state. The meticulous efforts to address missing values, standardize data formats, and rename columns have culminated in a dataset that is well-suited for further analysis and modeling.

#Isolating Numerics from Categorical Features

After meticulously cleaning and wrangling our data, we now embark on the crucial task of separating numerical features from categorical ones. This step is essential for applying appropriate machine learning techniques to each type of data.

Numerical features represent data that can be quantified using numbers. They embody measurable quantities or attributes, such as product energy, sugar content, or fat content. These variables play a pivotal role in predictive modeling tasks like linear regression, where we seek to establish relationships between numerical variables and a target variable.

Thus we create the following dataframe for our analyses.

**numerics_df**: This dataframe contains all numerical columns in df_cleaned_V1
"""

# create numerical features group
numerics_df = df_cleaned_V1.select_dtypes(include = 'number')

"""## Exploring Correlations
To gain a deeper understanding of the relationships between the numeric features in our dataset, we employ a correlation heatmap. This visualization technique effectively portrays the strength and direction of correlations among variables, providing valuable insights into the underlying structure of the data.

We begin by checking the correlations among all the columns in numerics_df. To do so, we create a correlation matrix using numerics_df and call it corr_mat. Next, using the correlation matrix corr_mat, we generated a correlation heatmap for these numeric features using the Seaborn library to generate a visually appealing heatmap.
"""

#create correlation matrix
corr_mat = numerics_df.corr()

#Create plot
##set figsize
fig, ax = plt.subplots(figsize=(6, 6))
##create heatmap
sns.heatmap(corr_mat, cmap = 'RdBu', vmin = -1, vmax = 1, center = 0)
plt.title('Correlation Heatmap of numerical variables')

plt.show()

"""By carefully examining the correlation heatmap above, we can identify patterns and trends in the relationships between numeric features. Strong positive correlations suggest that variables tend to move in the same direction, while strong negative correlations indicate opposite trends. These insights can inform our feature selection process, guiding us towards the most informative and impactful variables for our analysis.

Here, we see that the column "fat_100g" and other fats (saturated, monosaturated and polysaturated) are highly correlated because the former is a parent class. Thus, we will be excluding fat_100g in subsequent modeling tasks.

Sugars and carbohydrates while sharing stronger correlation, we cannot be sure that they represent the exact same things. Hence we are keeping it and will be addressing the correlation through other means.
"""

# drop the 'fat_100g' column
numerics_df = numerics_df.drop('fat_100g', axis = 1)

# check columns
numerics_df.columns

"""# Model Building and Data Analysis
With our data meticulously cleaned and prepared, we now venture into the realm of machine learning. Our journey begins with exploring the 'nutrition-score-uk_100g' column, a key indicator of product healthfulness.

To gain a comprehensive understanding of the distribution and characteristics of the 'nutrition-score-uk_100g' column, we employ the 'describe()' function to get a detailed summary of the column's statistical properties, including mean, standard deviation, percentiles, and minimum and maximum values.
"""

numerics_df['nutrition-score-uk_100g'].describe()

"""The Nutri-score system categorizes food into five distinct categories, ranging from A (most favorable) to E (least favorable). To align our analysis with this framework, we create a mapping between the numerical Nutri-score values and the corresponding letter categories."""

# Based on Nutri-Score formula, food are categorized in 5 categories.
df_cleaned_V1['label'] = pd.cut(x = df_cleaned_V1['nutrition-score-uk_100g'], bins=[-15, 0, 3, 11, 19, 40], labels = ['A','B','C','D','E'])

labels = df_cleaned_V1['label']

"""This categorization allows us to group products based on their overall nutritional profile, facilitating comparisons and identifying trends within different categories. The mapping ensures that our analysis adheres to the principles of the Nutri-score system, enabling meaningful interpretation of our findings.

We now proceed to split the data into two distinct subsets: a training set and a test set. This division is crucial for building and evaluating machine learning models, ensuring their generalizability and predictive power on unseen data.

To effectively split the data, we first define the features, which represent the input variables for our machine learning model. In this case, we assign the features to "numerics_df.drop('nutrition-score-uk_100g', axis=1)", excluding the target variable, 'nutrition-score-uk_100g'. This ensures that the model learns to predict the target variable based solely on the provided features.
"""

# set the features set
features = numerics_df.drop('nutrition-score-uk_100g', axis = 1)

"""Next, we define the labels, known as the target variable, which is what the model is trying to predict. We assign the target to "numerics_df['nutrition-score-uk_100g']", representing the Nutri-score category for each product."""

# set the target
target = numerics_df['nutrition-score-uk_100g']

"""Data splitting often involves a random element to ensure that the training and test sets are representative of the entire dataset. To achieve reproducible results, we set a seed value of 42 using the 'train_test_split()' function."""

# set the seed and split the train and test data
seed = 42
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.20, random_state=seed)

"""##Linear Regression
With our data now split into training and test sets, we can now move on to model building. Using the steps outlined in lecture, we begin by initializing and fitting a linear regression model using the "LinearRegression()" function. Our goal is to use the features defined earlier to predict the nutrition score for food products. Therefore, by initializing, fitting, and evaluating a linear regression model, we have taken a step towards understanding the factors that influence Nutri-score categories. In particular, the R-squared score provides valuable information about the model's ability to capture the underlying patterns in the data.
"""

#Initialize model with default parameters and fit it on the training set
reg = LinearRegression()
reg.fit(X_train, y_train)

# Use the model to predict on the test set and save these predictions as `y_pred`
y_pred = reg.predict(X_test)

# Find the R-squared score and store the value in `lin_reg_score`
lin_reg_score = sklearn.metrics.r2_score(y_test, y_pred)

lin_reg_score

"""The R-squared score of 0.6114403238660577 indicates that our linear regression model explains approximately 61.14% of the variance in the Nutri-score categories. This relatively high R-squared score suggests that the model effectively captures the underlying relationships between the numerical features and the target variable. Likewise, an R-squared score of 0.6114 is considered a moderate to good fit for linear regression models.

### Visualizing model performance for linear regression
To further assess the performance of our linear regression model, we create a scatter plot of the residuals. Residuals, the difference between the predicted values (y_pred) and the actual values (y_test), provide insights into the model's ability to capture the underlying patterns in the data.
"""

residuals = y_test - y_pred
plt.scatter(y_test, residuals)
plt.xlabel("Actual Values")
plt.ylabel("Residuals")
plt.axhline(y=0, color='r', linestyle='--', label="Residuals Mean")
plt.title("Residual Plot")
plt.legend()
plt.show()

"""The scatter plot reveals a some what linear trend. This may suggest that the linear regression model may not be adequately capturing the underlying relationships between the numerical features and the target variable, particularly for products with higher Nutri-score categories.

A possible explanation for this trend is that the model could be oversimplifying the complex relationships between the features and the target. For example, the model may not be accounting for the non-linear interactions between certain features. Despite this, the scatter plot shows that the majority of the residuals are within a reasonable range, which suggests that the model is generally accurate in its predictions.

##Logistic Regression
After successfully building a linear regression model to predict Nutri-score categories, we now venture into the realm of logistic regression. We aim to employ logistic regression to predict whether a product has a favorable Nutri-score category (A or B) or a less favorable Nutri-score category (C, D, or E).

Similar to linear regression, logistic regression requires a clear distinction between features and labels. We define the features to represent the numerical characteristics of the products that we believe influences the Nutri-score category. Likewise, we define the labels using 'df_cleaned_V1['label']' variable, where the labels indicate whether or not a product has a favorable Nutri-score category.
"""

features = numerics_df.drop('nutrition-score-uk_100g', axis = 1)
target = df_cleaned_V1['label']

seed = 42
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.20, random_state=seed)

# Initialize model with default parameters and fit it on the training set
clf = LogisticRegression()
clf.fit(X_train,y_train)

# Use the model to predict on the test set and save these predictions as `y_pred`
y_pred = clf.predict(X_test)

# Find the accuracy and store the value in `log_acc`
log_acc = sklearn.metrics.accuracy_score(y_pred,y_test)
print("Accuracy: %.1f%%"% (log_acc*100))

"""The obtained accuracy of 48% suggests that the model is performing moderately well in classifying products into favorable and less favorable Nutri-score categories. However, there is still room for improvement. We explore this further by creating confusion matrix.

### Using a confusion matrix to observe model performance
The confusion matrix is a valuable tool for evaluating the performance of classification models. It provides a detailed overview of the model's predictions, highlighting the number of correct and incorrect predictions made for each category.
"""

conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""[Revision needed]
Examining our confusion matrix above, we can see that we had 13260 correct predictions.

products with a favorable Nutri-score category were correctly predicted as having a favorable Nutri-score category. In other words, these were all true positives. On the other hand, our false positives were 9890 products with a less favorable Nutri-score category. They were incorrectly predicted as having a more favorable Nutri-score category.

In terms of true negatives, 1574 products with a less favorable Nutri-score category were correctly predicted as having a less favorable Nutri-score. And, 2686 products with a favorable Nutri-score were incorrectly predicted as having a less favorable Nutri-score.

The relatively high number of false positives suggests that the model is overestimating the number of products with favorable Nutri-score categories. This could be due to the model not adequately capturing the patterns and relationships between the features and the labels. Similarly, the number of false negatives is also relatively high, indicating that the model is underestimating the number of products with favorable Nutri-score categories.

##Logistic Regression with PCA
In our pursuit of a more robust and accurate model for predicting Nutri-score categories, we now employ the powerful technique of Principal Component Analysis (PCA). By reducing the dimensionality of the data, PCA effectively allows us to eliminate redundant and noisy features, focusing on the most informative components that contribute to the classification task. This noise reduction improves the signal-to-noise ratio, allowing the logistic regression model to capture the underlying patterns more accurately.

To get an idea of how many features we are working with, we get the number of columns using the len() function.
"""

len(features.columns)

"""Following the steps outlined in lecture, we begin by preparing the data through standardization and feature transformation. We want to ensure that the data is in a suitable form for PCA to effectively extract the underlying patterns and reduce the feature space while preserving the most informative components."""

scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

#Instantiate and Fit PCA
pca = PCA()
pca.fit(X_train_scaled)
pca.n_components_

"""To capture the explained variance ratios for each principal component, we store them in a variable named 'explained_variance_ratios'. This variable holds the individual explained variance ratios, providing a measure of the discriminative power of each component. Likewise, the cumulative explained variance ratio represents the cumulative proportion of variance explained by the first n principal components. It provides a holistic view of the explanatory power of the principal components."""

# Save the explained variance ratios into variable called "explained_variance_ratios"
explained_variance_ratios = pca.explained_variance_ratio_

# Save the CUMULATIVE explained variance ratios into variable called "cum_evr"
cum_evr = np.cumsum(pca.explained_variance_ratio_)

"""Next, we aim to find the optimal number of principal components to retain. This is a crucial decision that balances dimensionality reduction with information preservation."""

# find optimal num components to use (n) by plotting explained variance ratio
#set figure size
fig, ax = plt.subplots(figsize=(8, 6))
#set x-labels
x_labels = [i for i in range(1,31)]

#draw
sns.lineplot(
    data = cum_evr
)
plt.axhline(0.8, color = 'red')
plt.axvline(12)

ax.set_xlabel('Number of components')
ax.set_ylabel('Variance Explained')
ax.set_title('Explained variance ratio')
ax.set_xticks([i for i in range(0,30)],x_labels)

plt.show()

"""When examining the line plot above, we see that the first 13 components are able to explain 80% of the variance in the data. Retaining these 13 components would allow us to reduce the dimensionality of the data by 80% without losing much important information. This can lead to significant improvements in the efficiency and performance of the logistic regression model.
As a result, we proceed to train and fit PCA using 13 components.
"""

# Get transformed set of principal components on x_test

# 1. Refit and transform on training with parameter n (as deduced from the last step)
pca2 = PCA(n_components=13)

#pca.fit(X_train)
X_train_pca = pca2.fit_transform(X_train_scaled)

# 2. Transform on Testing Set and store it as `X_test_pca`
X_test_pca = pca2.transform(X_test_scaled)

# Initialize `log_reg_pca` model with default parameters and fit it on the PCA transformed training set
log_reg_pca = LogisticRegression()
log_reg_pca.fit(X_train_pca, y_train)

# Use the model to predict on the PCA transformed test set and save these predictions as `y_pred`
y_pred = log_reg_pca.predict(X_test_pca)

# Find the accuracy and store the value in `test_accuracy`
test_accuracy = sklearn.metrics.accuracy_score(y_pred,y_test)

test_accuracy

"""### An improvement in test accuracy
After we fit and train the PCA using 13 components, we see that the accuracy score has improved from 48% to 58.9%. This substantial improvement in test accuracy highlights the effectiveness of dimensionality reduction in enhancing the model's performance.

Next, we take a further look by creating a bar chart of explained variance ratio of principal components.
"""

plt.bar(range(1, len(pca2.explained_variance_ratio_) + 1), pca2.explained_variance_ratio_)
plt.xlabel("Principal Components")
plt.ylabel("Explained Variance Ratio")
plt.title("Explained Variance Ratio of Principal Components")
plt.show()

"""The bar chart above provides valuable insights into the underlying structure of the data and the relative importance of each principal component. It shows that the first component explains the largest portion of the variance in the data, followed by the second component, and so on. The variance explained by each component decreases rapidly for the first few components, but then levels off. This suggests that the first few components capture most of the important information in the data, while the remaining components contain less important information.

To optimize things further, let us take a second look at the columns in our dataframe.
"""

df_cleaned_V1.columns

"""### Drop one-hot columns with PCA

Since one-hot encoding introduces redundancy into the data, it can lead to multicollinearity, which can hinder the performance of machine learning models. By dropping one-hot columns and retaining only the most informative features through PCA, we aim to further improve the classification accuracy of the logistic regression model.

We begin by selecting the columns we want as features, and once again, perform the steps outlined in lecture.
"""

features = df_cleaned_V1[['additives', 'energy_100g', 'saturated-fat_100g',
       'monounsaturated-fat_100g', 'polyunsaturated-fat_100g',
       'trans-fat_100g', 'cholesterol_100g', 'carbohydrates_100g',
       'sugars_100g', 'fiber_100g', 'proteins_100g', 'salt_100g']]

seed = 42
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.20, random_state=seed)

scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

#Instantiate and Fit PCA
pca = PCA()
pca.fit(X_train_scaled)
pca.n_components_

# Save the explained variance ratios into variable called "explained_variance_ratios"
explained_variance_ratios = pca.explained_variance_ratio_

# Save the CUMULATIVE explained variance ratios into variable called "cum_evr"
cum_evr = np.cumsum(pca.explained_variance_ratio_)

# find optimal num components to use (n) by plotting explained variance ratio (2 points)
#set figure size
fig, ax = plt.subplots(figsize=(8, 6))
#set x-labels
x_labels = [i for i in range(1,12)]

#draw
sns.lineplot(
    data = cum_evr
)
plt.axhline(0.8, color = 'red')
plt.axvline(6)

ax.set_xlabel('Number of components')
ax.set_ylabel('Variance Explained')
ax.set_title('Explained variance ratio')
ax.set_xticks([i for i in range(0,11)],x_labels)

plt.show()

"""The line plot of explained variance ratio shows that the first 7 components explain 80% of the variance in the data. This is a remarkable finding, as it suggests that we can reduce the dimensionality of the data by 80% without losing too much important information. Therefore, by retaining only the first 7 principal components, we can develop a more efficient and robust model.

We proceed to train and fit the PCA on 7 components using what was taught in lecture.
"""

# Get transformed set of principal components on x_test

# 1. Refit and transform on training with parameter n (as deduced from the last step)
pca2 = PCA(n_components=7)

#pca.fit(X_train)
X_train_pca = pca2.fit_transform(X_train_scaled)

# 2. Transform on Testing Set and store it as `X_test_pca`
X_test_pca = pca2.transform(X_test_scaled)

# Initialize `log_reg_pca` model with default parameters and fit it on the PCA transformed training set
log_reg_pca = LogisticRegression()
log_reg_pca.fit(X_train_pca, y_train)

# Use the model to predict on the PCA transformed test set and save these predictions as `y_pred`
y_pred = log_reg_pca.predict(X_test_pca)

# Find the accuracy and store the value in `test_accuracy`
test_accuracy = sklearn.metrics.accuracy_score(y_pred,y_test)
test_accuracy

"""The slight decrease in test accuracy from 55.6% to 62.2% after applying PCA with 7 components suggests a trade-off between dimensionality reduction and information preservation. While PCA effectively reduces the dimensionality of the data, it may also eliminate some informative features, potentially affecting the model's ability to accurately classify unseen data. However, the overall accuracy of 62.2% still demonstrates the model's ability to make meaningful predictions, suggesting that PCA's benefits in terms of efficiency outweigh the slight reduction in accuracy.

Next, we continue by creating a confusion matrix to examine things further.
"""

cm = pd.DataFrame(confusion_matrix(y_test, y_pred, labels = ['A','B','C','D','E']))

cm

"""### Looking at model performance of PCA using confusion matrix
At this point, it would make sense to look at how our model is doing with PCA compared to earlier.
"""

#visualizing the confusion matrix
plt.figure(figsize = (8,4))
ax = sns.heatmap(cm, annot=True, annot_kws={"size": 8}, square=True, fmt = 'g', cmap = 'GnBu')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""Looking at the confusion matrix, we see that the overall accuracy of the model is modest. Interesting enough, the model is more accurate at predicting certain Nutri-score categories than others. We also examine that the model makes more false positives than false negatives.

We can use this information to improve the model by identifying and addressing the underlying causes of the errors. For example, additional features could be added to the model to help it distinguish between different Nutri-score categories.
"""

features = numerics_df.drop('nutrition-score-uk_100g', axis = 1)
target = df_cleaned_V1['label']

"""Furthermore, we see that the distribution of class labels is not even. This means we will need to resample the training data."""

df_cleaned_V1.groupby('label')['product_name'].count()

"""### Resample the training data
The uneven distribution of class labels, as evident from the confusion matrix, poses a challenge for the Nutri-score prediction model. To address this imbalance and ensure the model learns effectively from all classes, we employ the RandomOverSampler() technique. By addressing class imbalance and standardizing the features, we set the stage for a more robust and accurate Nutri-score prediction model.



"""

ros = RandomOverSampler()
X_ros, y_ros = ros.fit_resample(X_train, y_train)

# scale the data
scaler2 = StandardScaler()

X_ros_train_scaled = scaler2.fit_transform(X_ros)

X_ros_test_scaled = scaler2.transform(X_test)

# Get transformed set of principal components on x_test

# 1. Refit and transform on training with parameter n (as deduced from the last step)
pca2 = PCA(n_components=7)

# pca.fit(X_train)
X_ros_train_pca = pca2.fit_transform(X_ros_train_scaled)

# 2. Transform on Testing Set and store it as `X_test_pca`
X_ros_test_pca = pca2.transform(X_ros_test_scaled)

# Initialize `log_reg_pca` model with default parameters and fit it on the PCA transformed training set
log_reg_pca2 = LogisticRegression()
log_reg_pca2.fit(X_ros_train_pca, y_ros)

# Use the model to predict on the PCA transformed test set and save these predictions as `y_pred`
y_pred = log_reg_pca.predict(X_ros_test_pca)

# Find the accuracy and store the value in `test_accuracy`
test_accuracy = sklearn.metrics.accuracy_score(y_pred,y_test)
test_accuracy

"""The significant decrease in accuracy from 62.2% to 27.1% after applying RandomOverSampler might seem counterintuitive. While the goal of resampling is to improve model performance by addressing class imbalance, it can sometimes lead to a drop in accuracy. This can be due to number of factors such as increase in noise, overfitting, and increase in model complexity. While resampling is a valuable technique for addressing class imbalance, it is important to carefully evaluate its impact on model performance. In some cases, the trade-off between balanced class distribution and model complexity may not be favorable like in our case."""

cm2 = pd.DataFrame(confusion_matrix(y_test, y_pred, labels = ['A','B','C','D','E']))

"""### Another look at the confusion matrix after using RandomOverSampler()"""

#visualizing the confusion matrix
plt.figure(figsize = (8,4))
ax = sns.heatmap(cm2, annot=True, annot_kws={"size": 8}, square=True, fmt = 'g', cmap = 'GnBu')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""As alluded to earlier, the significant decrease in accuracy suggests that the resampling technique had a negative impact on the model's performance. We see that the number of false positives and false negatives increased for all Nutri-score categories after using RandomOverSampler. This suggests that the resampling technique made the model more likely to misclassify samples.

# Neural Network

Neural netweorks (NNs) are inspired by the structure and function of the human brain, consisting of interconnected layers of artificial neurons. These neurons process information, pass signals between each other, and learn from the data they are exposed to. This ability to learn complex patterns from data makes NNs particularly well-suited for tasks like Nutri-score prediction, where the underlying relationships between food composition and Nutri-score categories may be intricate.

Let us first begin by reminding ourselves how the dataframe df_cleaned_V1 looks like by calling the head() function.
"""

df_cleaned_V1.head()

"""Likewise, let us take a look at the "target" again as well."""

target

"""Now that we have an idea, we employ preprocessing steps to prepare the data for neural network modeling. These steps are crucial for ensuring that the data is in a suitable format for training and evaluating the neural network model."""

NN_df = df_cleaned_V1.select_dtypes(include = 'number').drop(['fat_100g', 'nutrition-score-uk_100g'], axis=1)

df_cleaned_V1['numeric_target'] = pd.cut(x = df_cleaned_V1['nutrition-score-uk_100g'], bins=[-15, 0, 3, 11, 19, 40], labels = [0,1,2,3,4])

df_cleaned_V1.astype({'numeric_target': 'int64'}).dtypes

"""We want to ensure that the target labels are readily accessible during the training and evaluation process of the neural network model. Likewise, we need to standardize the data types of all values in the NN_df dataFrame by converting to float32. This is a common practice in neural network modeling as most neural network algorithms expect input data to be in floating-point format."""

NN_df.insert(0, "label", df_cleaned_V1['numeric_target'])

NN_df = NN_df.astype(np.float32)

NN_df.dtypes

"""After carefully examining the dataframe, we notice the label column is the int32 datatype. Thus, we need to cast it to float32 as well.


"""

NN_df = NN_df.astype({'label': 'float32'}) #int32

"""## Setting up PyTorch
The creation of a PyTorch dataset and dataloader is a fundamental step in the process of building a neural network model. These tools play a crucial role in organizing, loading, and managing the data for efficient training and evaluation of the neural network. We begin this process using what was taught in lecture.
"""

#create pytorch dataset and dataloader
class CustomDataset(Dataset):
    def __init__(self, dataframe):
        self.dataframe = NN_df

    def __getitem__(self, index):
        row = self.dataframe.iloc[index].to_numpy()
        features = row[1:]
        label = row[0]
        return features, label

    def __len__(self):
        return len(self.dataframe)

NN_dataset = CustomDataset(NN_df)

#number of rows for 80% of dataset
NN_dataset_80 = int(np.floor(.8*len(NN_dataset)))

"""Now, we will split the NN_dataset into training and validation sets, ensuring that the model is trained on a representative sample of the data and evaluated on a separate, unseen sample."""

train_data, test_data = random_split(NN_dataset, [NN_dataset_80, len(NN_dataset) - NN_dataset_80])

"""We are ready to create our dataloader now."""

# Batch-size - a hyperparameter
batch = 64
train_loader = DataLoader(train_data, batch_size = batch, shuffle = True)
test_loader = DataLoader(test_data, batch_size = batch, shuffle = False)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('Using device:', device)

"""## Initializing neural network layers
Before diving into the training process, it's essential to properly set up the neural network architecture and prepare the image data for input into the model. This involves initializing the network layers according to the chosen architecture and converting the image data into a format that the neural network can effectively process.
"""

class LogReg(nn.Module):
    def __init__(self):
        super().__init__()
        # initialize the neural network layers
        # To flatten your images as vectors so that NN can read them
        # self.flatten = nn.Flatten()
        self.dropout = nn.Dropout(0.2)
        self.fc1 = nn.Linear(23, 23) #(64*23, 64)
        self.fc2 = nn.Linear(23, 10)
        self.fc3 = nn.Linear(10, 5)
        self.sigmoid = nn.Sigmoid()
        self.relu = nn.ReLU()
        self.softmax = nn.Softmax(dim=0)

    def forward(self, x):
        # implement the operations on input data
        # Hint: think of the neural network architecture for logistic regression
        # , self.softmax
        outputs = nn.Sequential(self.fc1, self.sigmoid, self.fc2, self.sigmoid, self.fc3, self.sigmoid, self.softmax)(x)
        return outputs

"""The flattening process ensures that the network can efficiently process the image information and extract relevant features for classification or regression tasks. The flattened image vector represents the image as a sequence of pixel intensity values, allowing the network to learn patterns and relationships within the image data.

Utilizing the LogReg Class for Initialization and Flattening
"""

LogReg()

"""## Evaluating the neural network performance
We proceed to calculate the training accuracy for each epoch using the steps outlined in lecture.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Sending the data to device (CPU or GPU)
# # TODO: (1 of 2)
# # Step 1: instantiate the logistic regression to variable logreg
# logreg = LogReg().to(device)
# 
# # Step 2: set the loss criterion as CrossEntropyLoss
# criterion = nn.CrossEntropyLoss()
# 
# # END TODO
# optimizer = optim.Adam(logreg.parameters(), lr=1e-3) #lr - learning step
# # optimizer = optim.SGD(logreg.parameters(), lr=1e-5, momentum=0.9)
# epoch = 50
# 
# loss_LIST_log = []
# acc_LIST_log = []
# 
# # Train the Logistic Regression
# for epoch in range(epoch):
#   running_loss = 0.0
#   correct = 0
#   total = 0
#   for inputs, labels in train_loader:
#       labels = labels.type(torch.LongTensor) # Cast to Float
#       inputs, labels = inputs.to(device), labels.to(device)
# 
#       ## TODO (2 of 2)
#       # Step 1: Reset the optimizer tensor gradient every mini-batch
#       optimizer.zero_grad()
# 
#       # Step 2: Feed the network the train data
#       outputs = logreg(inputs)
# 
#       # Step 3: Get the prediction using argmax
#       preds = torch.argmax(outputs, axis=1)
# 
#       # Step 4: Find average loss for one mini-batch of inputs
#       loss = criterion(outputs, labels)
# 
#       # Step 5: Do a back propagation
#       loss.backward()
# 
#       # Step 6: Update the weight using the gradients from back propagation by learning step
#       optimizer.step()
# 
#       # Step 7: Get loss and add to accumulated loss for each epoch
#       running_loss += loss.item() * len(labels)
# 
#       # Step 8: Get number of correct prediction and increment the number of correct and total predictions after this batch
#       # Hint: we need to detach the numbers from GPU to CPU, which stores accuracy and loss
#       correct += (preds == labels).sum().item()
#       total += len(preds)
# 
#   # Step 9: Calculate training accuracy for each epoch (should multiply by 100 to get percentage), store in variable called 'accuracy', and add to acc_LIST_log
#   accuracy = correct / len(train_data) * 100
#   acc_LIST_log.append(accuracy)
# 
#   # Step 10: Get average loss for each epoch and add to loss_LIST_log
#   avg_loss = running_loss / len(train_data)
#   loss_LIST_log.append(avg_loss)
# 
#   # print statistics
#   print("The loss for Epoch {} is: {}, Accuracy = {}".format(epoch, running_loss/len(train_loader), accuracy))
#

"""Looking at the results above, we were able to achieve an accuracy of 85%. This means that the neural network correctly classifies approximately 85% of the test data samples. While this accuracy is decent, it suggests that the model can still be improved to better distinguish between different classes. Likewise, the average loss of 102 indicates that the model has some error in its predictions. To further optimize the neural network, we can consider tweaking the hyperparameters like learning rate and batch size to determine the best configuration for the model. Next, we employ visualization to help us look at the bigger picture when it comes to loss.

## Analysis of our neural network
We use visualization to help us analyze what is going on in our neural network during training so that we can perhaps make some changes to further optimize it. We start with looking at the training loss over epochs.
"""

# Plotting the Loss
plt.figure(figsize=(10, 5))
plt.plot(range(epoch + 1), loss_LIST_log, label='Training Loss')
plt.title('Training Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""Looking at the plot above, we can see the training loss over epochs, which represents the average error between the model's predictions and the ground truth labels for each epoch. As the model trains, the training loss curve in the plot shows a significant decrease in loss in the initial epochs. This suggests that the model is quickly learning the underlying patterns in the data and improving its predictive performance. But, in the same way, the loss curve plateaus, indicating that the model is struggling to further improve its performance."""

# Plotting the Accuracy
plt.figure(figsize=(10, 5))
plt.plot(range(epoch + 1), acc_LIST_log, label='Training Accuracy')
plt.title('Training Accuracy Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.legend()
plt.show()

"""We see that the training accuracy curve shows the percentage of correctly classified training samples for each epoch. As the model trains, the training accuracy curve in the provided plot shows a steady increase in accuracy over epochs. This suggests that the model is learning the data effectively and improving its predictive performance. On the other hand, the accuracy curve saturates in the later epochs, indicating that the model has reached its optimal capacity for learning the training data. This is a good sign, as it suggests that the model has learned the underlying patterns in the data and can be used to make predictions on unseen data.

#Decision tree
"""

features = numerics_df.drop('nutrition-score-uk_100g', axis = 1)
target = df_cleaned_V1['label']
seed = 42
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.20, random_state=seed)

from sklearn.tree import DecisionTreeClassifier
import sklearn
import matplotlib.pyplot as plt
accuracy_list = []
for i in range(1, 25):
  # Training step, on X_train with y_train
  clf = DecisionTreeClassifier(criterion="entropy", max_depth=i, random_state=seed)
  clf = clf.fit(X_train, y_train)

  # Prediction step, with X_test (and we will validate accuracy
  # against y_test)
  prediction = clf.predict(X_test)

  # Test accuracy, ie Jaccard distance of matched items
  accuracy = sklearn.metrics.accuracy_score(prediction,y_test)
  accuracy_list.append(accuracy*100)
  print("Accuracy: %.1f%%"% (accuracy*100))

# Plotting the Accuracy
plt.figure(figsize=(10, 5))
plt.plot(range(1,25), accuracy_list, label='Training Accuracy')
plt.axhline(80, color = 'red')
plt.title('Training Accuracy Over Tree Depth')
plt.xticks(range(1,25))
plt.xlabel('Depth')
plt.ylabel('Accuracy (%)')
plt.legend()
plt.show()

# Training step, on X_train with y_train
clf = DecisionTreeClassifier(criterion="entropy", max_depth=8, random_state=seed)
clf = clf.fit(X_train, y_train)

# Prediction step, with X_test (and we will validate accuracy
# against y_test)
prediction = clf.predict(X_test)

# Test accuracy, ie Jaccard distance of matched items
accuracy = sklearn.metrics.accuracy_score(prediction,y_test)
print("Accuracy: %.1f%%"% (accuracy*100))

sklearn.metrics.roc_auc_score(y_test, clf.predict_proba(X_test), average= 'macro', multi_class = 'ovr')

import graphviz
from sklearn.tree import export_graphviz
import pydotplus

plt.figure(figsize=(12, 8))
plot_tree(clf, feature_names=features.columns, class_names=clf.classes_, filled=True, rounded=True)
plt.show()

'''
plt.figure(figsize=(16, 10))
plot_tree(clf, feature_names=features.columns, class_names=clf.classes_, filled=True, rounded=True, fontsize=12)

for index, value in enumerate(clf.classes_):
    plt.text(index, 1.5, f"{index}: {value}", horizontalalignment='center', verticalalignment='center', fontdict={'weight': 'bold'})

plt.show()
'''
'''
dot_data = export_graphviz(clf, out_file=None, feature_names=features.columns, class_names=clf.classes_, filled=True, rounded=True)

graph = pydotplus.graph_from_dot_data(dot_data)

graph.set_graph_defaults(fontname="Arial", fontsize=10, ranksep="0.1", nodesep="0.1")
graphviz.Source(dot_data).view()
'''

from matplotlib import pyplot as plt

plt.figure(dpi=400)
tree.plot_tree(clf)

from sklearn.ensemble import RandomForestClassifier
accuracy_list = []

for depth in range(1,11):
  clf = RandomForestClassifier(n_estimators = 20, max_depth=depth)
  clf.fit(X_train,y_train)
  prediction = clf.predict(X_test)

  accuracy = sklearn.metrics.accuracy_score(prediction,y_test)
  accuracy_list.append(accuracy*100)
  print("Accuracy: %.1f%%"% (accuracy*100))

plt.plot(range(1, 11), accuracy_list, marker='o')
plt.title('Random Forest Classifier Accuracy vs. Depth')
plt.xlabel('Max Depth')
plt.ylabel('Accuracy (%)')
plt.grid(True)
plt.show()

"""#Ensemble"""

from sklearn.ensemble import AdaBoostClassifier
tree = DecisionTreeClassifier(criterion = 'entropy',
                              max_depth = 2,
                              random_state = seed)

ada = AdaBoostClassifier(base_estimator = tree,
                         n_estimators = 500,
                         learning_rate = 0.1,
                         random_state = seed)

ada.fit(X_train,y_train)
prediction = ada.predict(X_test)

accuracy = sklearn.metrics.accuracy_score(prediction,y_test)
print("Accuracy: %.1f%%"% (accuracy*100))

print("ROC AUC score: ", sklearn.metrics.roc_auc_score(y_test, ada.predict_proba(X_test), average= 'macro', multi_class = 'ovr'))

"""# Conclusion"""