# -*- coding: utf-8 -*-
"""The Glam Guys.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KNag-uIyJwU9gNpITqiCP2hBsp4f-I5n

# Introduction / Background

**Introduction:**

Open Food Facts is a non-profit collaborative project that aims to create a comprehensive, open-source database of food products from around the world. The project is managed by a non-profit organization with over a hundred thousand active contributors. The fact that the database is open source means that anyone willing to volunteer can contribute. Currently, the database contains information on over 3 million food products from 150 countries with information pertaining to the ingredients, allergens, and nutrition facts found on product labels. The data is collected from a variety of sources, including barcode scans, product labels, and volunteer contributions.


**Background on the dataset:**

The Open Food Facts database was created out of frustration with the lack of transparency in the food industry. Because food manufacturers often use proprietary ingredients and complex labeling, many consumers are ill-informed when it comes to knowing what is in their food. Thus, the Open Food Facts project aims to provide transparency to consumers with easy to access product information that is accurate and up-to-date. ðŸ™‚


**Project Objective:**

Using the Open Food Facts database, we want to achieve the following objectives in this project:

1.  Predict the UK Nutrition score categories of a food product by using Linear regression model
2.   Group different food products based on their ingredients by using K-means clustering

Before starting the analysis, we import the necessary libraries as follows:

Loading Dependencies
"""

!pip install torchinfo

# Commented out IPython magic to ensure Python compatibility.
# import packages

# to mount google drive
from google.colab import drive

# For data analysis
import pandas as pd
import numpy as np

# For data visualization
from matplotlib import pyplot as plt
# %matplotlib inline
import seaborn as sns

# sklearn libraries
import sklearn
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
import sklearn.metrics
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import confusion_matrix

# resampling package
from imblearn.over_sampling import RandomOverSampler

import torch
import torchvision
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchinfo import summary
from torch.utils.data import random_split

# Setting the device to do computations on - GPU's are generally faster!
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(torch.__version__)
print(device)

"""Mount Google Drive, import data and read data into a Panda DataFrame"""

# drive.flush_and_unmount()
drive.mount('/content/drive')
df = pd.read_table('/content/drive/MyDrive/en.openfoodfacts.org.products.tsv')
#df = pd.read_table('/content/drive/MyDrive/5450/en.openfoodfacts.org.products.tsv')
# df = pd.read_table('/content/drive/MyDrive/kaggle/en.openfoodfacts.org.products.tsv')

"""# Exploratory Data Analysis

The Open Food Facts dataset is quite large with a size of about 1 GB (~360000 entries!). So, it would make sense to first conduct EDA to understand what we are working with. To have an idea, let us take a look at the first 10 rows of our dataframe df.

"""

# Peek first 10 rows of the data set
df.head(10)

"""## What are the data types?
From our initial observations, we can see that the dataframe contains information on a variety of food products, including soft drinks, teas, and other beverages. The products are from a variety of countries, including the United States, Germany, Spain, the United Kingdom, and France. The dataframe also includes information on the nutrients in each product, such as energy, sugar, and fat per 100 grams. Likewise, there are columns that list the ingredients and allergens (if applicable) for each product. On the other hand, there are plenty of columns that the Open Food Facts website uses as part of their backend code such as when a product entry was created ("created_datetime") or when it was last modified ("last_modified_datetime"). These columns are not particularly useful to us. However, columns like("product_name") and ("ingredients_text") will definitely be of interest.

Next, we shall take a look at what sort of data types we are dealing with in the columns of this dataframe.
"""

# Check data types of columns and the shape of dataframe
df.dtypes

"""## What is the shape?
We can observe that the data types are quite diverse, consisting of objects and float64. Since a majority of the columns contain object values, this indicates that the dataset incorporates a variety of data formats, including strings (product name, categorie, country), numerical data (nutrients per 100 grams), and even comma-separated values (ingredients).

The presence of comma-separated values within the "ingredients_text" column suggests that the nutritional information for each product is some what structured and organized, allowing us the potential to analyze and explore nutritional patterns and trends. As a result, this means we will need to cast the data types appropriately so that it can be parsed.

Now, we will take a look at the shape of this dataframe and the descriptive summary.
"""

# Get the shape of the dataframe
df.shape

"""## A descriptive summary of our dataframe
As alluded to earlier in our introduction, the Open Food Facts dataset is quite large and comprehensive with a shape that is an astonishing 356027 rows and 163 columns! This tells us that the dataset has a significant amount of information on a wide range of food products. The large number of rows implies that the dataset can potentially provide detailed analysis and insights into the various aspects of food products, such as their origins, pricing, and nutritional composition. Likewise, the large number of columns suggests that the dataset covers a broad range of attributes related to each food product.

Then, we shall take a look at the descriptive summary of this dataframe.
"""

# Get a descriptive summary of the dataframe
df.describe()

'''
selected_feature = 'nutrition-score-fr_100g'


df['nutrition-score-fr_100g'].value_counts().plot(kind='bar')
plt.title(f'Distribution of {selected_feature}')
plt.xlabel(selected_feature)
plt.ylabel('Count')
plt.show()
'''

#Data visualization
##bar graph
##why we pick one feature over the others
##show rationale in picking features
##correlation matrix

"""## What columns do we have?
Since there are 163 columns in the dataframe, there is no way we can display them all at once when viewing the dataframe. So, one way is to iterate all the columns in the dataframe and print them out.
"""

# Look through all the available columns in the dataset
for col in df.columns:
  print(col)

"""## Selecting relevant columns

Selecting relevant columns is a crucial step to ensure that the analysis focuses on the most informative and pertinent data. Given our logistic regression and clustering tasks, we are primarily interested in the ingredients of the food products, as these provide insights into the nutritional properties and potential health implications of the products. Therefore, we are dropping columns that are not useful for the tasks such as data entry and packaging information; only keeping ingredients-related columns.

By focusing on ingredients-related columns, we can effectively streamline the analysis process, reducing computational complexity and improving the efficiency of our models. Moreover, excluding irrelevant columns helps to minimize noise and potential biases that may arise from unrelated data, enhancing the accuracy and reliability of our results.

We will create a new dataframe called new_df by selecting the columns we want. Then, we will cast the columns containing objects to string data type.
"""

# Select relevant columns
new_df = df[['product_name', 'brands','brands_tags', 'countries_en', 'ingredients_text', 'allergens', 'additives_n', 'additives_en', 'energy_100g', 'fat_100g', 'saturated-fat_100g', 'monounsaturated-fat_100g', 'polyunsaturated-fat_100g', 'omega-3-fat_100g', 'omega-6-fat_100g', 'omega-9-fat_100g', 'trans-fat_100g', 'cholesterol_100g', 'carbohydrates_100g', 'sugars_100g', 'starch_100g', 'polyols_100g', 'fiber_100g', 'proteins_100g', 'salt_100g', 'fruits-vegetables-nuts_100g', 'nutrition-score-uk_100g']]

# Cast types of columns and confirm change
new_df['product_name'] = new_df['product_name'].astype('string')
new_df['brands'] = new_df['brands'].astype('string')
new_df['brands_tags'] = new_df['brands_tags'].astype('string')
new_df['countries_en'] = new_df['countries_en'].astype('string')
new_df['ingredients_text'] = new_df['ingredients_text'].astype('string')
new_df['allergens'] = new_df['allergens'].astype('string')
new_df['additives_en'] = new_df['additives_en'].astype('string')
new_df = new_df.sort_values(by='additives_n', ascending=False)

"""We can now take a quick look at the result of our new dataframe."""

# Check the data types for the columns
new_df.dtypes

# Sense check cleaned df
new_df.info()

"""At this point, it makes sense to take a look at the first 10 rows of our dataframe new_df to get a sense of how things look. We have not dropped any null values yet, so it is expected that there will be some rows with this problem."""

# Print the first 10 rows of new_df
new_df.head(10)

"""To limit ourselves on English data, we narrow down the dataset to only entries from the United States and where the ingredient list is not empty."""

# Filter the rows for products that are in the United States
new_1_df = new_df[new_df['countries_en'] == 'United States']

# Drop any rows that contain null as the ingredients
new_1_df.dropna(subset=['ingredients_text'], inplace=True)

# Display the data frame new_1_df
new_1_df

"""With our dataframe new_1_df containing the information needed, we are ready to move onto cleaning and wrangling the data!

# Data Cleaning and Wrangling
We took a comprehensive approach to data cleaning and wrangling, addressing common data quality issues and preparing the data for subsequent analyses. We achieve this in the following steps:

**Cleaned Dataframe: df_cleaned_V1** size:(170551, 29)

**(1)** Checking NaN distribution among all columns;\
The first step is to identify and handle any missing values (NaNs) across all columns.

**(2)** Padding numerical columns with zeros for all NaN entries;\
We check the NaN distribution across all columns and pad any numerical columns with zeros for NaN entries. This ensures that numerical columns have consistent representations, preventing errors in computations and ensuring effective ML training.

**(3)** Remove rows with NaNs in alphabatical entries;\
For string-based columns, we remove rows with NaNs in alphabetical entries. This helps maintain data integrity and prevent any inconsistencies that could arise from incomplete or inaccurate information.

**(4)** Convert Non-English letters to English letters in alphabatical columns;\
Typically, non-English characters are common in string-based columns. For this reason, we convert any non-English letters to English letters in alphabetical columns. This ensures consistent data representation and facilitates accurate text analysis.

**(5)** Expand ingredients column into several one-hot coding columns padding with 1s and 0s;\
We will transform the "ingredients_text" column into one-hot coding columns, effectively representing the data in a numerical format suitable for ML training. This step allows the ingredients information to be incorporated into the analysis in a structured and interpretable manner.

**(6)** Rename some column names and ready data for following ML training and testing.
We will rename the column names to be more descriptive and meaningful, which will enhance the readability and interpretability of the data. This improves the clarity of the data and facilitates easier understanding for subsequent analyses.

We begin with step (1) by checking the distribution of NaN values across all of the columns.
"""

# Check NaNs value's means
new_1_df.isnull().mean(axis=0)

"""## Proportion of NaN values
The proportion of NaN values can be calculated by getting the mean for NaN values in the dataset. The resulting percentage represents the proportion of data points that are missing for that particular column.

With respect to the Open Food Facts dataset, examining the proportion of NaN values can help us understand the completeness of the information provided for various product attributes. For example, if a significant proportion of values are missing for a column like "ingredients_text", it may indicate that the data collection process needs improvement or that there are limitations in the availability of ingredient information for some products.

Thus, to help us quickly identify columns with high proportions of missing data, we use visualization techniques such bar charts to represent these proportions.
"""

# check the proportion of NaNs in each column
plt.figure(figsize=(5, 10))
new_1_df.isnull().mean(axis=0).plot.barh()
plt.title("Proportion of NaNs in each column")

"""## Why is there missing data?
Upon examining the bar chart above, we can see that several columns exhibit a high proportion of NaN values. These columns include "allergens", "omega-3-fat_100g", "omega-6-fat_100g", "omega-9-fat_100g", "starch_100g", "polyols_100g", and "fruits-vegetables-nuts_100g".


The high proportion of NaN values in the "allergens" column suggests that allergen information is not consistently provided for all products. We believe this is due to variations in product labeling practices or a lack of standardized allergen reporting guidelines. Likewise, the missing values in the omega fatty acid columns "omega-3-fat_100g", "omega-6-fat_100g", and "omega-9-fat_100g" suggests that information about omega fatty acid content is not always available. This could be due to the complexity of measuring omega fatty acids or a lack of emphasis on this nutritional aspect by some manufacturers. Similarly, the absence of starch content data in the "starch_100g" column suggests that this information may not be routinely collected or reported for all food products. Perhaps, there are variations in starch content across different product types or a lack of standardized starch measurement protocols. Overall, we believe there are no standard guidelines when it comes to product labelling practices, which may contribute to the inconsistencies in the dataset.


Thus, the presence of missing data in these columns have many implications for the analysis and interpretation of the Open Food Facts dataset. If we aim to analyze the prevalence of allergens or the nutritional composition of products, the missing values could introduce biases and limit the accuracy of our findings. For this reason, we will drop columns with 100% NaN values.
"""

new_1_df.columns

# drop columns with 100% NaNs
useless_features = ["fruits-vegetables-nuts_100g", "polyols_100g", "starch_100g", "omega-3-fat_100g", "omega-6-fat_100g", "omega-9-fat_100g", "allergens"]
df_cleaned = new_1_df.drop(useless_features, axis=1, inplace=False)

# Sense check new dataframe df_cleaned
print(df_cleaned.shape)
print(df_cleaned.columns)

"""## Replacing NaN values with zeroes appropriately
We assume that NaN values in some columns represent the absence of the corresponding ingredients. We feel this is a reasonable assumption given the context of the dataset. Since the Open Food Facts dataset aims to provide comprehensive information about food products, it is likely that NaN values indicate that the ingredients are not present rather than simply missing due to data collection errors or inconsistencies.

Replacing NaN values with zeroes effectively imputes these missing values by assuming that the ingredient is absent and therefore quantifying its presence as zero. This approach is particularly useful for numerical columns representing quantities of ingredients, such as "fat_100g" or "sugars_100g".

By replacing NaN values with zeroes, we are able to incorporate these columns into our analysis. This allows us to make more informed comparisons between products and identify patterns in ingredient usage. However, we acknowledge that this imputation strategy assumes that the absence of an ingredient is the only possible explanation for a NaN value.
"""

# replace NaNs with zeros in numerical columns
df_cleaned["additives_n"] = df_cleaned["additives_n"].fillna(0)
df_cleaned["energy_100g"] = df_cleaned["energy_100g"].fillna(0)
df_cleaned["fat_100g"] = df_cleaned["fat_100g"].fillna(0)
df_cleaned["saturated-fat_100g"] = df_cleaned["saturated-fat_100g"].fillna(0)
df_cleaned["monounsaturated-fat_100g"] = df_cleaned["monounsaturated-fat_100g"].fillna(0)
df_cleaned["polyunsaturated-fat_100g"] = df_cleaned["polyunsaturated-fat_100g"].fillna(0)
df_cleaned["trans-fat_100g"] = df_cleaned["trans-fat_100g"].fillna(0)
df_cleaned["cholesterol_100g"] = df_cleaned["cholesterol_100g"].fillna(0)
df_cleaned["carbohydrates_100g"] = df_cleaned["carbohydrates_100g"].fillna(0)
df_cleaned["sugars_100g"] = df_cleaned["sugars_100g"].fillna(0)
df_cleaned["fiber_100g"] = df_cleaned["fiber_100g"].fillna(0)
df_cleaned["proteins_100g"] = df_cleaned["proteins_100g"].fillna(0)
df_cleaned["salt_100g"] = df_cleaned["salt_100g"].fillna(0)
df_cleaned["nutrition-score-uk_100g"] = df_cleaned["nutrition-score-uk_100g"].fillna(0)
df_cleaned.head()

# check the proportion of NaNs in each column
plt.figure(figsize=(5, 10))
df_cleaned.isnull().mean(axis=0).plot.barh()
plt.title("Proportion of NaNs in each column")

# drop additives_en column, over 35% data is NaNs
useless_features = ["additives_en"]
df_cleaned = df_cleaned.drop(useless_features, axis=1, inplace=False)

# drop rows containing NaNs
df_cleaned = df_cleaned.dropna()

# check for NaNs, complete.
plt.figure(figsize=(5, 10))
df_cleaned.isnull().mean(axis=0).plot.barh()
plt.title("Proportion of NaNs in each column")

# check for cleaned dataset dimensions
print(df_cleaned.shape)
df_cleaned.head()

!pip install easynmt

# convert all non-english letters into english letters in alphabatical columns
# convert all tags into lower-case, in order for following ML training use
df_cleaned_V1 = df_cleaned.copy()
df_cleaned_V1["product_name"] = df_cleaned_V1["product_name"].str.normalize('NFKD').str.encode('ascii',errors='ignore').str.decode('utf-8').str.lower()
df_cleaned_V1["brands"] = df_cleaned_V1["brands"].str.normalize('NFKD').str.encode('ascii',errors='ignore').str.decode('utf-8').str.lower()
df_cleaned_V1["brands_tags"] = df_cleaned_V1["brands_tags"].str.normalize('NFKD').str.encode('ascii',errors='ignore').str.decode('utf-8').str.lower()
df_cleaned_V1["countries_en"] = df_cleaned_V1["countries_en"].str.normalize('NFKD').str.encode('ascii',errors='ignore').str.decode('utf-8').str.lower()
df_cleaned_V1["ingredients_text"] = df_cleaned_V1["ingredients_text"].str.normalize('NFKD').str.encode('ascii',errors='ignore').str.decode('utf-8').str.lower()

df_cleaned_V1.head()

np.sum(df_cleaned_V1["brands"]!=df_cleaned_V1["brands_tags"])

# drop brands_tags column, it's same with brands column
useless_features = ["brands_tags"]
df_cleaned_V1 = df_cleaned_V1.drop(useless_features, axis=1, inplace=False)
df_cleaned_V1.head()

# rename some columns
df_cleaned_V1 = df_cleaned_V1.rename(columns={"countries_en": "countries", "ingredients_text": "ingredients", "additives_n": "additives"})

# explode ingredients column
# add columns indicating whether some ingredients are in this product
# for future ML use, feel free to join more columns here
# list(df_cleaned_V1['ingredients'].str.split(' ', expand=True).stack().unique())
df_cleaned_V1["has_flour"] = df_cleaned_V1["ingredients"].str.contains("flour").apply(lambda x: 1 if x==True else 0)
df_cleaned_V1["has_sugar"] = df_cleaned_V1["ingredients"].str.contains("sugar").apply(lambda x: 1 if x==True else 0)
df_cleaned_V1["has_water"] = df_cleaned_V1["ingredients"].str.contains("water").apply(lambda x: 1 if x==True else 0)
df_cleaned_V1["has_salt"] = df_cleaned_V1["ingredients"].str.contains("salt").apply(lambda x: 1 if x==True else 0)
df_cleaned_V1["has_potassium"] = df_cleaned_V1["ingredients"].str.contains("potassium").apply(lambda x: 1 if x==True else 0)
df_cleaned_V1["has_calcium"] = df_cleaned_V1["ingredients"].str.contains("calcium").apply(lambda x: 1 if x==True else 0)
df_cleaned_V1["has_acid"] = df_cleaned_V1["ingredients"].str.contains("acid").apply(lambda x: 1 if x==True else 0)
df_cleaned_V1["has_alkali"] = df_cleaned_V1["ingredients"].str.contains("alkali").apply(lambda x: 1 if x==True else 0)
df_cleaned_V1["has_iodine"] = df_cleaned_V1["ingredients"].str.contains("iodine").apply(lambda x: 1 if x==True else 0)
df_cleaned_V1["has_milk"] = df_cleaned_V1["ingredients"].str.contains("milk").apply(lambda x: 1 if x==True else 0)
df_cleaned_V1["has_oil"] = df_cleaned_V1["ingredients"].str.contains("oil").apply(lambda x: 1 if x==True else 0)
df_cleaned_V1.head()

# data ready for modeling
print(df_cleaned_V1.shape)
print(df_cleaned_V1.dtypes)

"""#Isolating Numerics from Categorical Features

Create groups of the numeric and categorical variables

numerics_df: This dataframe contains all numerical columns in df_cleaned_V1 to be used in Linear Regression

one_hot_df: This dataframe contains all columns of type 'int64'; one-hot encoded columns created earlier to be used for clustering

categorical_df: This dataframe contains all categorical columns from df_cleaned_V1
"""

numerics_df = df_cleaned_V1.select_dtypes(include = 'number')
categorical_df = df_cleaned_V1.select_dtypes(include = 'object')

"""Next, we check the correlations among all the columns in numerics_df using correlation heatmap.
First, we create a correlation matrix using numerics_df and call it corr_mat. Using the correlation matrix, we generated a correlation heatmap for these numeric features using Seaborn library.
"""

#create correlation matrix
corr_mat = numerics_df.corr()

#Create plot
##set figsize
fig, ax = plt.subplots(figsize=(6, 6))
##create heatmap
sns.heatmap(corr_mat, cmap = 'RdBu', vmin = -1, vmax = 1, center = 0)
plt.title('Correlation Heatmap of numerical variables')

plt.show()

"""Here we see that fat_100g and other fats (saturated, monosaturated and polysaturated) as highly correlated because the former is a parent class. Thus we will be excluding fat_100g in subsequent modeling tasks."""

numerics_df = numerics_df.drop('fat_100g', axis = 1)

numerics_df.columns

"""# Model Building and Data Analysis"""

numerics_df['nutrition-score-uk_100g'].describe()

#Based on Nutri-Score formula, food are categorized in 5 categories.
df_cleaned_V1['label'] = pd.cut(x = df_cleaned_V1['nutrition-score-uk_100g'], bins=[-15, 0, 3, 11, 19, 40], labels = ['A','B','C','D','E'])

##Linear
##K-means clustering

#Comparison of different models
##NN
##random forest decision tree

#measuring of performance
##10 classes -
##precision & recall

#findings and conclusions along the way

"""Now that we have explored and cleaned our dataset, here we create Features and Labels. We also split the data into Train and Test sets."""

features = numerics_df.drop('nutrition-score-uk_100g', axis = 1)

target = numerics_df['nutrition-score-uk_100g']

seed = 42
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.20, random_state=seed)

"""##Linear Regression
Here we are using the features defined earlier to predict the nutrition-score-uk_100g.
"""

#Initialize model with default parameters and fit it on the training set
reg = LinearRegression()
reg.fit(X_train, y_train)

# TO-DO: Use the model to predict on the test set and save these predictions as `y_pred`
y_pred = reg.predict(X_test)

# TO-DO: Find the R-squared score and store the value in `lin_reg_score`
lin_reg_score = sklearn.metrics.r2_score(y_test, y_pred)

lin_reg_score

"""From the R2 score, we can see that the features are able to explain about 61% of the variance in the UK nutrition score."""

residuals = y_test - y_pred
plt.scatter(y_test, residuals)
plt.xlabel("Actual Values")
plt.ylabel("Residuals")
plt.axhline(y=0, color='r', linestyle='--', label="Residuals Mean")
plt.title("Residual Plot")
plt.legend()
plt.show()

"""##Logistic Regression
Here we are predicting the UK Nutrition categories using the features.
"""

features = numerics_df.drop('nutrition-score-uk_100g', axis = 1)
target = df_cleaned_V1['label']

seed = 42
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.20, random_state=seed)

# TO-DO: Initialize model with default parameters and fit it on the training set
clf = LogisticRegression()
clf.fit(X_train,y_train)

# TO-DO: Use the model to predict on the test set and save these predictions as `y_pred`
y_pred = clf.predict(X_test)

# TO-DO: Find the accuracy and store the value in `log_acc`
log_acc = sklearn.metrics.accuracy_score(y_pred,y_test)
print("Accuracy: %.1f%%"% (log_acc*100))

conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""##Logistic Regression with PCA
We want to reduce noise by using PCA to improve logistic regression.
"""

len(features.columns)

scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

#Instantiate and Fit PCA
pca = PCA()
pca.fit(X_train_scaled)
pca.n_components_

# TO-DO: Save the explained variance ratios into variable called "explained_variance_ratios"
explained_variance_ratios = pca.explained_variance_ratio_

# TO-DO: Save the CUMULATIVE explained variance ratios into variable called "cum_evr"
cum_evr = np.cumsum(pca.explained_variance_ratio_)

# TO-DO: find optimal num components to use (n) by plotting explained variance ratio (2 points)
#set figure size
fig, ax = plt.subplots(figsize=(8, 6))
#set x-labels
x_labels = [i for i in range(1,31)]

#draw
sns.lineplot(
    data = cum_evr
)
plt.axhline(0.8, color = 'red')
plt.axvline(12)

ax.set_xlabel('Number of components')
ax.set_ylabel('Variance Explained')
ax.set_title('Explained variance ratio')
ax.set_xticks([i for i in range(0,30)],x_labels)

plt.show()

"""We see that using 13 components is able to explain 80% of the variance.
Below, we train and fit PCA using 13 components.
"""

# TO-DO: Get transformed set of principal components on x_test

# 1. Refit and transform on training with parameter n (as deduced from the last step)
pca2 = PCA(n_components=13)

#pca.fit(X_train)
X_train_pca = pca2.fit_transform(X_train_scaled)

# 2. Transform on Testing Set and store it as `X_test_pca`
X_test_pca = pca2.transform(X_test_scaled)

# TO-DO: Initialize `log_reg_pca` model with default parameters and fit it on the PCA transformed training set
log_reg_pca = LogisticRegression()
log_reg_pca.fit(X_train_pca, y_train)

# TO-DO: Use the model to predict on the PCA transformed test set and save these predictions as `y_pred`
y_pred = log_reg_pca.predict(X_test_pca)

# TO-DO: Find the accuracy and store the value in `test_accuracy`
test_accuracy = sklearn.metrics.accuracy_score(y_pred,y_test)

test_accuracy

"""We see that the accuracy score has improved from 49.9% to 55.6% using PCA."""

plt.bar(range(1, len(pca2.explained_variance_ratio_) + 1), pca2.explained_variance_ratio_)
plt.xlabel("Principal Components")
plt.ylabel("Explained Variance Ratio")
plt.title("Explained Variance Ratio of Principal Components")
plt.show()

"""Drop One-hot columns with PCA"""

df_cleaned_V1.columns

features = df_cleaned_V1[['additives', 'energy_100g', 'saturated-fat_100g',
       'monounsaturated-fat_100g', 'polyunsaturated-fat_100g',
       'trans-fat_100g', 'cholesterol_100g', 'carbohydrates_100g',
       'sugars_100g', 'fiber_100g', 'proteins_100g', 'salt_100g']]

seed = 42
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.20, random_state=seed)

scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

#Instantiate and Fit PCA
pca = PCA()
pca.fit(X_train_scaled)
pca.n_components_

# TO-DO: Save the explained variance ratios into variable called "explained_variance_ratios"
explained_variance_ratios = pca.explained_variance_ratio_

# TO-DO: Save the CUMULATIVE explained variance ratios into variable called "cum_evr"
cum_evr = np.cumsum(pca.explained_variance_ratio_)

# TO-DO: find optimal num components to use (n) by plotting explained variance ratio (2 points)
#set figure size
fig, ax = plt.subplots(figsize=(8, 6))
#set x-labels
x_labels = [i for i in range(1,12)]

#draw
sns.lineplot(
    data = cum_evr
)
plt.axhline(0.8, color = 'red')
plt.axvline(6)

ax.set_xlabel('Number of components')
ax.set_ylabel('Variance Explained')
ax.set_title('Explained variance ratio')
ax.set_xticks([i for i in range(0,11)],x_labels)

plt.show()

# TO-DO: Get transformed set of principal components on x_test

# 1. Refit and transform on training with parameter n (as deduced from the last step)
pca2 = PCA(n_components=7)

#pca.fit(X_train)
X_train_pca = pca2.fit_transform(X_train_scaled)

# 2. Transform on Testing Set and store it as `X_test_pca`
X_test_pca = pca2.transform(X_test_scaled)

# TO-DO: Initialize `log_reg_pca` model with default parameters and fit it on the PCA transformed training set
log_reg_pca = LogisticRegression()
log_reg_pca.fit(X_train_pca, y_train)

# TO-DO: Use the model to predict on the PCA transformed test set and save these predictions as `y_pred`
y_pred = log_reg_pca.predict(X_test_pca)

# TO-DO: Find the accuracy and store the value in `test_accuracy`
test_accuracy = sklearn.metrics.accuracy_score(y_pred,y_test)
test_accuracy

cm = pd.DataFrame(confusion_matrix(y_test, y_pred, labels = ['A','B','C','D','E']))

cm

#visualizing the confusion matrix
plt.figure(figsize = (8,4))
ax = sns.heatmap(cm, annot=True, annot_kws={"size": 8}, square=True, fmt = 'g', cmap = 'GnBu')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

features = numerics_df.drop('nutrition-score-uk_100g', axis = 1)
target = df_cleaned_V1['label']

"""We see that the distribution of class labels is not even. We need so do resample."""

df_cleaned_V1.groupby('label')['product_name'].count()

"""Resampling the training data and train the model"""

ros = RandomOverSampler()
X_ros, y_ros = ros.fit_resample(X_train, y_train)

#scale the data
scaler2 = StandardScaler()

X_ros_train_scaled = scaler2.fit_transform(X_ros)

X_ros_test_scaled = scaler2.transform(X_test)

# TO-DO: Get transformed set of principal components on x_test

# 1. Refit and transform on training with parameter n (as deduced from the last step)
pca2 = PCA(n_components=7)

#pca.fit(X_train)
X_ros_train_pca = pca2.fit_transform(X_ros_train_scaled)

# 2. Transform on Testing Set and store it as `X_test_pca`
X_ros_test_pca = pca2.transform(X_ros_test_scaled)

# TO-DO: Initialize `log_reg_pca` model with default parameters and fit it on the PCA transformed training set
log_reg_pca2 = LogisticRegression()
log_reg_pca2.fit(X_ros_train_pca, y_ros)

# TO-DO: Use the model to predict on the PCA transformed test set and save these predictions as `y_pred`
y_pred = log_reg_pca.predict(X_ros_test_pca)

# TO-DO: Find the accuracy and store the value in `test_accuracy`
test_accuracy = sklearn.metrics.accuracy_score(y_pred,y_test)
test_accuracy

cm2 = pd.DataFrame(confusion_matrix(y_test, y_pred, labels = ['A','B','C','D','E']))

#visualizing the confusion matrix
plt.figure(figsize = (8,4))
ax = sns.heatmap(cm2, annot=True, annot_kws={"size": 8}, square=True, fmt = 'g', cmap = 'GnBu')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""Neural Network"""

df_cleaned_V1.head()

target

NN_df = df_cleaned_V1.select_dtypes(include = 'number').drop(['fat_100g', 'nutrition-score-uk_100g'],axis=1)

df_cleaned_V1['numeric_target'] = pd.cut(x = df_cleaned_V1['nutrition-score-uk_100g'], bins=[-15, 0, 3, 11, 19, 40], labels = [0,1,2,3,4])

df_cleaned_V1.astype({'numeric_target': 'int64'}).dtypes

NN_df.insert(0, "label", df_cleaned_V1['numeric_target'])

NN_df = NN_df.astype(np.float32)

NN_df.dtypes

NN_df = NN_df.astype({'label': 'float32'}) #int32

#create pytorch dataset and dataloader
class CustomDataset(Dataset):
    def __init__(self, dataframe):
        self.dataframe = NN_df

    def __getitem__(self, index):
        row = self.dataframe.iloc[index].to_numpy()
        features = row[1:]
        label = row[0]
        return features, label

    def __len__(self):
        return len(self.dataframe)

NN_dataset = CustomDataset(NN_df)

#number of rows for 80% of dataset
np.floor(.8*170551)

train_data, test_data = random_split(NN_dataset, [136440, len(NN_dataset) - 136440])

# Batch-size - a hyperparameter
batch = 64
train_loader = DataLoader(train_data, batch_size = batch, shuffle = True)
test_loader = DataLoader(test_data, batch_size = batch, shuffle = False)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('Using device:', device)

class LogReg(nn.Module):
    def __init__(self):
        super().__init__()
        # TODO: initialize the neural network layers
        # To flatten your images as vectors so that NN can read them
        # self.flatten = nn.Flatten()
        self.dropout = nn.Dropout(0.2)
        self.fc1 = nn.Linear(23, 23) #(64*23, 64)
        self.fc2 = nn.Linear(23, 10)
        self.fc3 = nn.Linear(10, 5)
        self.sigmoid = nn.Sigmoid()
        self.relu = nn.ReLU()
        self.softmax = nn.Softmax(dim=0)

    def forward(self, x):
        # TODO: implement the operations on input data
        # Hint: think of the neural network architecture for logistic regression
        # , self.softmax
        outputs = nn.Sequential(self.fc1, self.sigmoid, self.fc2, self.sigmoid, self.fc3, self.sigmoid, self.softmax)(x)
        return outputs

LogReg()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Sending the data to device (CPU or GPU)
# # TODO: (1 of 2)
# # Step 1: instantiate the logistic regression to variable logreg
# logreg = LogReg().to(device)
# 
# # Step 2: set the loss criterion as CrossEntropyLoss
# criterion = nn.CrossEntropyLoss()
# 
# # END TODO
# optimizer = optim.Adam(logreg.parameters(), lr=1e-3) #lr - learning step
# # optimizer = optim.SGD(logreg.parameters(), lr=1e-5, momentum=0.9)
# epoch = 50
# 
# loss_LIST_log = []
# acc_LIST_log = []
# 
# # Train the Logistic Regression
# for epoch in range(epoch):
#   running_loss = 0.0
#   correct = 0
#   total = 0
#   for inputs, labels in train_loader:
#       labels = labels.type(torch.LongTensor) # Cast to Float
#       inputs, labels = inputs.to(device), labels.to(device)
# 
#       ## TODO (2 of 2)
#       # Step 1: Reset the optimizer tensor gradient every mini-batch
#       optimizer.zero_grad()
# 
#       # Step 2: Feed the network the train data
#       outputs = logreg(inputs)
# 
#       # Step 3: Get the prediction using argmax
#       preds = torch.argmax(outputs, axis=1)
# 
#       # Step 4: Find average loss for one mini-batch of inputs
#       loss = criterion(outputs, labels)
# 
#       # Step 5: Do a back propagation
#       loss.backward()
# 
#       # Step 6: Update the weight using the gradients from back propagation by learning step
#       optimizer.step()
# 
#       # Step 7: Get loss and add to accumulated loss for each epoch
#       running_loss += loss.item() * len(labels)
# 
#       # Step 8: Get number of correct prediction and increment the number of correct and total predictions after this batch
#       # Hint: we need to detach the numbers from GPU to CPU, which stores accuracy and loss
#       correct += (preds == labels).sum().item()
#       total += len(preds)
# 
#   # Step 9: Calculate training accuracy for each epoch (should multiply by 100 to get percentage), store in variable called 'accuracy', and add to acc_LIST_log
#   accuracy = correct / len(train_data) * 100
#   acc_LIST_log.append(accuracy)
# 
#   # Step 10: Get average loss for each epoch and add to loss_LIST_log
#   avg_loss = running_loss / len(train_data)
#   loss_LIST_log.append(avg_loss)
# 
#   # print statistics
#   print("The loss for Epoch {} is: {}, Accuracy = {}".format(epoch, running_loss/len(train_loader), accuracy))
#

# Plotting the Loss
plt.figure(figsize=(10, 5))
plt.plot(range(epoch + 1), loss_LIST_log, label='Training Loss')
plt.title('Training Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Plotting the Accuracy
plt.figure(figsize=(10, 5))
plt.plot(range(epoch + 1), acc_LIST_log, label='Training Accuracy')
plt.title('Training Accuracy Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.legend()
plt.show()